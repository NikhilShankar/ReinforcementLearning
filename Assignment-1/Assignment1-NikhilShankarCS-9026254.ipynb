{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Reinforcement Learning Programming - CSCN 8020**\n",
    "#### **Assignment-1**\n",
    "##### **Prepared by**\n",
    "- Nikhil Shankar C S\n",
    "- 9026254"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Problem 1**\n",
    "\n",
    "##### **Markovs Decision Process**\n",
    "\n",
    "**States** \n",
    "- S0 = Idle\n",
    "- S1 = Move\n",
    "- S2 = Pick\n",
    "- S3 = Place\n",
    "\n",
    "**Actions**\n",
    "- A0 = Detects object\n",
    "- A1 = Moves toward object\n",
    "- A2 = Picks up the object\n",
    "- A3 = Places the object\n",
    "- A5 = Returns to rest.\n",
    "\n",
    "**Rewards**\n",
    "- R0 = 0\n",
    "- R1 = f(S0, A0, S1) = 10\n",
    "- R2 = f(S1, A1, S2) = 10\n",
    "- R3 = f(S2, A2, S3) = 10\n",
    "- R4 = f(S3, A3, S4) = 10\n",
    "- R5 = f(S4, A4, S5) = 10\n",
    "- R.unknown = f(S.n, A.n, S.x) = -100  ( This last reward function factors for an Action A.n in A0-A5 resulting in an Unknown state or error. This means whenever the picked object is not correctly placed or drops it would result in a negative reward)\n",
    "> Note : f(S.n, A.n, S.n+1) is the reward function for doing doing Action A.n from State S.n to S.n+1\n",
    "\n",
    "So MDP for simple pick and place robot can be defined as above:\n",
    "\n",
    "We can add a transition function denoting the probability of state changes and actions\n",
    "\n",
    "**Transition Function :**\n",
    "- P(S.n|S.n+1, A.n) = 0.9 **if** S.n+1 is the intended state **else** 0.1 if S.n+1 is unintended state.\n",
    "> The above transition function is a naive and basic transition function. We can ideally create a probability function representing errors and abnormalities that can happen at each stage of transition. \n",
    "\n",
    "###### **We can also define a Discount factor:** \n",
    "**Discount Factor = 0.2**\n",
    "> This will make sure that the system appreciates immediate rewards over future rewards, which is best for a pick and place robot Since each step is crucial in the final outcome.\n",
    "Discount factor hence can be assigned as 0.2 so that current reward is given highest priority.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Problem 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initial Policy is given - always take action UP\n",
    "- Keeping Discount Factor Gamma = 1 : This makes it easy for calculations.\n",
    "- Starting with initial Value as 0 for all states.\n",
    "\n",
    "- Initial Policy says that we can only move up in the first transition: \n",
    "\n",
    "#### **Iteration 1**\n",
    "\n",
    "##### **Value for S1**\n",
    "- --> Move up from S1 > end up in S1 > Reward = 5\n",
    "- --> Since initial value of all states are zero > Value(S1) = 5 + (1*0) = 5\n",
    "\n",
    "##### **Value for S2**\n",
    "- --> Move up from S2 > End up in S2 > Reward = 10\n",
    "- --> Since initial value of all states are zero > Value(S2) = 10 + (1*0) = 10\n",
    "\n",
    "##### **Value for S3**\n",
    "- --> Move up from S3 > End up in S1 > Reward = 5\n",
    "- --> Since initial value of all states are zero > Value(S3) = 5 + (1*0) = 5\n",
    "\n",
    "##### **Value for S4**\n",
    "- --> Move up from S4 > End up in S2 > Reward = 10\n",
    "- --> Since initial value of all states are zero > Value(S4) = 10 + (1*0) = 10\n",
    "\n",
    "- After 1st Iteration Values are\n",
    "\n",
    "|  |  |\n",
    "| -- | -- |\n",
    "| **5** | **10** |\n",
    "| **5** | **10** |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Iteration 2**\n",
    "\n",
    "- After first iteration we can choose to do any action from the states\n",
    "\n",
    "##### **Value for S1 - Iter2**\n",
    "\n",
    "- **Option Left** \n",
    "- --> Move left from S1 > End up in S1 > Reward = 5\n",
    "- --> Since previous value of S1 = 5 and Gamma = 1 > Value(S1) = 5 + (1 * 5) = 10\n",
    "\n",
    "- **Option Right** \n",
    "- --> Move right from S1 > End up in S2 > Reward = 10\n",
    "- --> Since previous value of S2 = 10 and Gamma = 1 > Value(S1) = 10 + (1 * 10) = 20\n",
    "\n",
    "- **Option Up** \n",
    "- --> Move up from S1 > End up in S1 > Reward = 5\n",
    "- --> Since previous value of S1 = 5 and Gamma = 1 > Value(S1) = 5 + (1 * 5) = 10\n",
    "\n",
    "- **Option Down** \n",
    "- --> Move down from S1 > End up in S3 > Reward = 1\n",
    "- --> Since previous value of S3 = 5 and Gamma = 1 > Value(S1) = 1 + (1 * 5) = 6\n",
    "\n",
    "- Maximum out of all options\n",
    "- Max( 10, 20, 10, 6) = 20\n",
    "\n",
    "- **So Value(S1) after Iteration 2 = 20**\n",
    "\n",
    "\n",
    "##### **Value for S2 - Iter2**\n",
    "\n",
    "- **Option Left** \n",
    "- --> Move left from S2 > End up in S1 > Reward = 5\n",
    "- --> Since previous value of S1 = 5 and Gamma = 1 > Value(S2) = 5 + (1 * 5) = 10\n",
    "\n",
    "- **Option Right** \n",
    "- --> Move right from S2 > End up in S2 > Reward = 10\n",
    "- --> Since previous value of S2 = 10 and Gamma = 1 > Value(S2) = 10 + (1 * 10) = 20\n",
    "\n",
    "- **Option Up** \n",
    "- --> Move up from S2 > End up in S2 > Reward = 10\n",
    "- --> Since previous value of S2 = 10 and Gamma = 1 > Value(S2) = 10 + (1 * 10) = 20\n",
    "\n",
    "- **Option Down** \n",
    "- --> Move down from S2 > End up in S4 > Reward = 2\n",
    "- --> Since previous value of S4 = 10 and Gamma = 1 > Value(S2) = 2 + (1 * 10) = 12\n",
    "\n",
    "- Maximum out of all options\n",
    "- Max( 10, 20, 20, 12) = 20\n",
    "\n",
    "- **So Value(S2) after Iteration 2 = 20**\n",
    "\n",
    "\n",
    "\n",
    "##### **Value for S3 - Iter2**\n",
    "\n",
    "- **Option Left** \n",
    "- --> Move left from S3 > End up in S3 > Reward = 1\n",
    "- --> Since previous value of S3 = 5 and Gamma = 1 > Value(S3) = 1 + (1 * 5) = 6\n",
    "\n",
    "- **Option Right** \n",
    "- --> Move right from S3 > End up in S4 > Reward = 2\n",
    "- --> Since previous value of S4 = 10 and Gamma = 1 > Value(S3) = 2 + (1 * 10) = 12\n",
    "\n",
    "- **Option Up** \n",
    "- --> Move up from S3 > End up in S1 > Reward = 5\n",
    "- --> Since previous value of S1 = 5 and Gamma = 1 > Value(S3) = 5 + (1 * 5) = 10\n",
    "\n",
    "- **Option Down** \n",
    "- --> Move down from S3 > End up in S3 > Reward = 1\n",
    "- --> Since previous value of S3 = 5 and Gamma = 1 > Value(S3) = 1 + (1 * 5) = 6\n",
    "\n",
    "- Maximum out of all options\n",
    "- Max( 6, 12, 10, 6) = 12\n",
    "\n",
    "- **So Value(S3) after Iteration 2 = 12**\n",
    "\n",
    "\n",
    "\n",
    "##### **Value for S4 - Iter2**\n",
    "\n",
    "- **Option Left** \n",
    "- --> Move left from S4 > End up in S3 > Reward = 1\n",
    "- --> Since previous value of S3 = 5 and Gamma = 1 > Value(S4) = 1 + (1 * 5) = 6\n",
    "\n",
    "- **Option Right** \n",
    "- --> Move right from S4 > End up in S4 > Reward = 2\n",
    "- --> Since previous value of S4 = 10 and Gamma = 1 > Value(S4) = 2 + (1 * 10) = 12\n",
    "\n",
    "- **Option Up** \n",
    "- --> Move up from S4 > End up in S2 > Reward = 10\n",
    "- --> Since previous value of S2 = 10 and Gamma = 1 > Value(S4) = 10 + (1 * 10) = 20\n",
    "\n",
    "- **Option Down** \n",
    "- --> Move down from S4 > End up in S4 > Reward = 2\n",
    "- --> Since previous value of S4 = 10 and Gamma = 1 > Value(S4) = 2 + (1 * 10) = 12\n",
    "\n",
    "- Maximum out of all options\n",
    "- Max( 6, 12, 20, 12) = 20\n",
    "\n",
    "- **So Value(S3) after Iteration 2 = 20**\n",
    "\n",
    "#### **Final Values after Iteration 2 with Gamma ( discount factor of 1)**\n",
    "\n",
    "|  |  |\n",
    "| -- | -- |\n",
    "| **20** | **20** |\n",
    "| **12** | **20** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Problem 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Baseline Code from Repo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "#!/usr/bin/python3\n",
    "import numpy as np\n",
    "\n",
    "class GridWorld():\n",
    "    def __init__(self, env_size):\n",
    "        self.env_size = env_size\n",
    "        # TODO: Change the location of the terminal state and check how the optimal policy changes\n",
    "        # TODO: Add more than one terminal state (requires more changes in the code)\n",
    "        self.terminal_state = (4, 4)\n",
    "\n",
    "        # Define the transition probabilities and rewards\n",
    "        self.actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # Right, Left, Down, Up\n",
    "        self.action_description = [\"Right\", \"Left\", \"Down\", \"Up\"]\n",
    "\n",
    "        # Assign a vector of rewards for each of the states\n",
    "        self.reward = np.ones((self.env_size, self.env_size))*-1\n",
    "        self.reward[self.terminal_state] = 0\n",
    "\n",
    "    '''@brief Returns the next state given the chosen action and current state\n",
    "    '''\n",
    "    def step(self, action_index, i, j):\n",
    "        # We are assuming a Transition Probability Matrix where\n",
    "        # P(s'|s) = 1.0 for a single state and 0 otherwise\n",
    "        action = self.actions[action_index]\n",
    "        next_i, next_j = i + action[0], j + action[1]\n",
    "        if not self.is_valid_state(next_i, next_j):\n",
    "            next_i, next_j = i, j\n",
    "        \n",
    "        done = self.is_terminal_state(next_i, next_j)\n",
    "        reward = self.reward[next_i, next_j]\n",
    "        return next_i, next_j, reward, done\n",
    "    \n",
    "    '''@brief Checks if a state is within the acceptable bounds of the environment\n",
    "    '''\n",
    "    def is_valid_state(self, i, j):\n",
    "        valid = 0 <= i < self.env_size and 0 <= j < self.env_size\n",
    "        return valid\n",
    "    \n",
    "    '''@brief Returns True if the state is a terminal state\n",
    "    '''\n",
    "    def is_terminal_state(self, i, j):\n",
    "        return (i, j) == self. terminal_state\n",
    "    \n",
    "    def get_size(self):\n",
    "        return self.env_size\n",
    "\n",
    "    def get_actions(self):\n",
    "        return self.actions\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, env, theta_threshold=0.01):\n",
    "        self.env_size = env.get_size()\n",
    "        self.env = env\n",
    "        # Initialize the value function and set terminal state value to 0\n",
    "        self.V = np.zeros((self.env_size, self.env_size))\n",
    "        # TODO: Change the location of the terminal state and check how the optimal policy changes\n",
    "        # TODO: Add more than one terminal state (requires more changes in the code)\n",
    "        self.terminal_state = (4, 4)\n",
    "        self.V[self.terminal_state] = 0\n",
    "\n",
    "        self.theta_threshold = theta_threshold\n",
    "\n",
    "        # Define the transition probabilities and rewards\n",
    "        self.actions = env.get_actions()  # Right, Left, Down, Up\n",
    "        self.gamma = 1.0  # Discount factor\n",
    "        self.pi_greedy = np.zeros((self.env_size, self.env_size), dtype=int)\n",
    "\n",
    "    '''@brief Calculate the maximim value by following a greedy policy\n",
    "    '''\n",
    "    def calculate_max_value(self, i, j):\n",
    "        # Find the maximum value for the current state using Bellman's equation\n",
    "        # Start with a - infinite value as the max\n",
    "        max_value = float('-inf')\n",
    "        best_action = None\n",
    "        best_actions_str = \"\"\n",
    "        # Loop over all actions\n",
    "        for action_index in range(len(self.actions)):\n",
    "          # Find Next state\n",
    "            next_i, next_j, reward, _ = self.env.step(action_index, i, j)\n",
    "            if self.env.is_valid_state(next_i, next_j):\n",
    "                value = self.get_value(next_i, next_j, reward)\n",
    "                if value >= max_value:\n",
    "                    # Populating the best_actions description string\n",
    "                    if value > max_value:\n",
    "                        best_actions_str = self.env.action_description[action_index]\n",
    "                    else:\n",
    "                        best_actions_str += \"|\" + self.env.action_description[action_index]\n",
    "\n",
    "                    best_action = action_index\n",
    "                    max_value = value\n",
    "        return max_value, best_action, best_actions_str\n",
    "    \n",
    "    '''@brief use the Bellman equation to calculate the value of a single state\n",
    "            Note that the equation is simplified due to the simple transition matrix\n",
    "    '''\n",
    "    def get_value(self, i, j, reward):\n",
    "        return reward + self.gamma * self.V[i, j]\n",
    "    \n",
    "    '''\n",
    "    @brief Overwrites the current state-value function with a new one\n",
    "    '''\n",
    "    def update_value_function(self, V):\n",
    "        self.V = np.copy(V)\n",
    "\n",
    "    '''\n",
    "    @brief Returns the full state-value function V_pi\n",
    "    '''\n",
    "    def get_value_function(self):\n",
    "        return self.V\n",
    "\n",
    "    '''@brief Finds the optimal action for every state and updates the policy\n",
    "    '''\n",
    "    def update_greedy_policy(self):\n",
    "        # Note: We are assuming a greedy deterministic policy\n",
    "        self.pi_str = []\n",
    "        for i in range(self.env_size):\n",
    "            pi_row = []\n",
    "            for j in range(self.env_size):\n",
    "                if self.env.is_terminal_state(i,j):\n",
    "                    pi_row.append(\"X\")\n",
    "                    continue\n",
    "                    \n",
    "                _, self.pi_greedy[i,j], action_str = self.calculate_max_value(i, j)\n",
    "                pi_row.append(action_str)\n",
    "            self.pi_str.append(pi_row)\n",
    "        \n",
    "    '''@brief Checks if there is the change in V is less than preset threshold\n",
    "    '''\n",
    "    def is_done(self, new_V):\n",
    "        delta = abs(self.V - new_V)\n",
    "        max_delta = delta.max()\n",
    "        return max_delta <= self.theta_threshold\n",
    "    \n",
    "    '''@brief Returns the stored greedy policy\n",
    "    '''\n",
    "    def get_policy(self):\n",
    "        return self.pi_greedy\n",
    "    \n",
    "    '''@brief Prints the policy using the action descriptions\n",
    "    '''\n",
    "    def print_policy(self):\n",
    "        for row in self.pi_str:\n",
    "            print(row)\n",
    "\n",
    "\n",
    "def run_mdp():\n",
    "    ENV_SIZE = 5\n",
    "    THETA_THRESHOLD = 0.05\n",
    "    MAX_ITERATIONS = 1000\n",
    "    env = GridWorld(ENV_SIZE)\n",
    "    agent = Agent(env, THETA_THRESHOLD)\n",
    "\n",
    "    # Perform value iteration\n",
    "    done = False\n",
    "    for iter in range(MAX_ITERATIONS):\n",
    "        # Add stopping criteria if change in value function is small\n",
    "        if done: break\n",
    "        # Make a copy of the value function\n",
    "        # TODO: Try in-place state-value function update where Vpi is updated with every state\n",
    "        new_V = np.copy(agent.get_value_function())\n",
    "        # Loop over all states\n",
    "        for i in range(ENV_SIZE):\n",
    "            for j in range(ENV_SIZE):\n",
    "                if not env.is_terminal_state(i, j):\n",
    "                    new_V[i, j], _, _= agent.calculate_max_value(i,j)\n",
    "        # TODO: Uncomment the next line and compare how many iterations it takes\n",
    "        # TODO: Change the theta_threshold value to a large value (1.0) and explore what happens to the optimal state-value function and policy\n",
    "        # done = agent.is_done(new_V)\n",
    "        agent.update_value_function(new_V)\n",
    "\n",
    "    # Print the optimal value function\n",
    "    print(\"Optimal Value Function Found in %d iterations:\"%(iter+1))\n",
    "    print(agent.get_value_function())\n",
    "\n",
    "    agent.update_greedy_policy()\n",
    "    agent.print_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Value Function Found in 1000 iterations:\n",
      "[[-7. -6. -5. -4. -3.]\n",
      " [-6. -5. -4. -3. -2.]\n",
      " [-5. -4. -3. -2. -1.]\n",
      " [-4. -3. -2. -1.  0.]\n",
      " [-3. -2. -1.  0.  0.]]\n",
      "['Right|Down', 'Right|Down', 'Right|Down', 'Right|Down', 'Down']\n",
      "['Right|Down', 'Right|Down', 'Right|Down', 'Right|Down', 'Down']\n",
      "['Right|Down', 'Right|Down', 'Right|Down', 'Right|Down', 'Down']\n",
      "['Right|Down', 'Right|Down', 'Right|Down', 'Right|Down', 'Down']\n",
      "['Right', 'Right', 'Right', 'Right', 'X']\n"
     ]
    }
   ],
   "source": [
    "run_mdp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Baseline Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Value Function Found in 1000 iterations:\n",
      "[[-7. -6. -5. -4. -3.]\n",
      " [-6. -5. -4. -3. -2.]\n",
      " [-5. -4. -3. -2. -1.]\n",
      " [-4. -3. -2. -1.  0.]\n",
      " [-3. -2. -1.  0.  0.]]\n",
      "['Right|Down', 'Right|Down', 'Right|Down', 'Right|Down', 'Down']\n",
      "['Right|Down', 'Right|Down', 'Right|Down', 'Right|Down', 'Down']\n",
      "['Right|Down', 'Right|Down', 'Right|Down', 'Right|Down', 'Down']\n",
      "['Right|Down', 'Right|Down', 'Right|Down', 'Right|Down', 'Down']\n",
      "['Right', 'Right', 'Right', 'Right', 'X']\n",
      "Execution Time: 0.544410 seconds\n",
      "Peak Memory Usage: 0.014391 MB\n"
     ]
    }
   ],
   "source": [
    "import tracemalloc\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "tracemalloc.start()  # Start memory tracking\n",
    " # Record start time\n",
    "result = run_mdp()\n",
    "end_time = time.time()  # Record end time\n",
    "current, peak = tracemalloc.get_traced_memory()\n",
    "tracemalloc.stop()\n",
    "\n",
    "print(f\"Execution Time: {end_time - start_time:.6f} seconds\")\n",
    "print(f\"Peak Memory Usage: {peak / 1024 / 1024:.6f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Task1.1 : Update MDP Code**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please note that there is an error in the question. The figure in the question has grey areas depicted at (3,0) (1,2) and (0,4) but the reward function definition has it defined as S2,2 S3,0 and S0,4 as Grey areas. **I am taking the reward function definition as the source of truth.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "#!/usr/bin/python3\n",
    "import numpy as np\n",
    "\n",
    "class GridWorld():\n",
    "    def __init__(self, env_size):\n",
    "        self.env_size = env_size\n",
    "        # TODO: Change the location of the terminal state and check how the optimal policy changes\n",
    "        # TODO: Add more than one terminal state (requires more changes in the code)\n",
    "        self.terminal_state = (4, 4)\n",
    "\n",
    "        # Define the transition probabilities and rewards\n",
    "        self.actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # Right, Left, Down, Up\n",
    "        self.action_description = [\"Right\", \"Left\", \"Down\", \"Up\"]\n",
    "\n",
    "        \n",
    "\n",
    "        ####### EDITED FOR THE TASK - Nikhil Shankar#####\n",
    "        ## Incorporating various rewards for the states\n",
    "        # Assign a vector of rewards for each of the states\n",
    "        self.reward = np.ones((self.env_size, self.env_size))*-1\n",
    "        self.reward[self.terminal_state] = 10 \n",
    "        self.reward[(2,2)] = -5\n",
    "        self.reward[(3,0)] = -5\n",
    "        self.reward[(0,4)] = -5\n",
    "        ####### EDITED FOR THE TASK - Nikhil Shankar#####\n",
    "\n",
    "    '''@brief Returns the next state given the chosen action and current state\n",
    "    '''\n",
    "    def step(self, action_index, i, j):\n",
    "        # We are assuming a Transition Probability Matrix where\n",
    "        # P(s'|s) = 1.0 for a single state and 0 otherwise\n",
    "        action = self.actions[action_index]\n",
    "        next_i, next_j = i + action[0], j + action[1]\n",
    "        if not self.is_valid_state(next_i, next_j):\n",
    "            next_i, next_j = i, j\n",
    "        \n",
    "        done = self.is_terminal_state(next_i, next_j)\n",
    "        reward = self.reward[next_i, next_j]\n",
    "        return next_i, next_j, reward, done\n",
    "    \n",
    "    '''@brief Checks if a state is within the acceptable bounds of the environment\n",
    "    '''\n",
    "    def is_valid_state(self, i, j):\n",
    "        valid = 0 <= i < self.env_size and 0 <= j < self.env_size\n",
    "        return valid\n",
    "    \n",
    "    '''@brief Returns True if the state is a terminal state\n",
    "    '''\n",
    "    def is_terminal_state(self, i, j):\n",
    "        return (i, j) == self. terminal_state\n",
    "    \n",
    "    def get_size(self):\n",
    "        return self.env_size\n",
    "\n",
    "    def get_actions(self):\n",
    "        return self.actions\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, env, theta_threshold=0.01):\n",
    "        self.env_size = env.get_size()\n",
    "        self.env = env\n",
    "        # Initialize the value function and set terminal state value to 0\n",
    "        self.V = np.zeros((self.env_size, self.env_size))\n",
    "        # TODO: Change the location of the terminal state and check how the optimal policy changes\n",
    "        # TODO: Add more than one terminal state (requires more changes in the code)\n",
    "        self.terminal_state = (4, 4)\n",
    "        self.V[self.terminal_state] = 0\n",
    "\n",
    "        self.theta_threshold = theta_threshold\n",
    "\n",
    "        # Define the transition probabilities and rewards\n",
    "        self.actions = env.get_actions()  # Right, Left, Down, Up\n",
    "        self.gamma = 1.0  # Discount factor\n",
    "        self.pi_greedy = np.zeros((self.env_size, self.env_size), dtype=int)\n",
    "\n",
    "    '''@brief Calculate the maximim value by following a greedy policy\n",
    "    '''\n",
    "    def calculate_max_value(self, i, j):\n",
    "        # Find the maximum value for the current state using Bellman's equation\n",
    "        # Start with a - infinite value as the max\n",
    "        max_value = float('-inf')\n",
    "        best_action = None\n",
    "        best_actions_str = \"\"\n",
    "        # Loop over all actions\n",
    "        for action_index in range(len(self.actions)):\n",
    "          # Find Next state\n",
    "            next_i, next_j, reward, _ = self.env.step(action_index, i, j)\n",
    "            if self.env.is_valid_state(next_i, next_j):\n",
    "                value = self.get_value(next_i, next_j, reward)\n",
    "                if value >= max_value:\n",
    "                    # Populating the best_actions description string\n",
    "                    if value > max_value:\n",
    "                        best_actions_str = self.env.action_description[action_index]\n",
    "                    else:\n",
    "                        best_actions_str += \"|\" + self.env.action_description[action_index]\n",
    "\n",
    "                    best_action = action_index\n",
    "                    max_value = value\n",
    "        return max_value, best_action, best_actions_str\n",
    "    \n",
    "    '''@brief use the Bellman equation to calculate the value of a single state\n",
    "            Note that the equation is simplified due to the simple transition matrix\n",
    "    '''\n",
    "    def get_value(self, i, j, reward):\n",
    "        return reward + self.gamma * self.V[i, j]\n",
    "    \n",
    "    '''\n",
    "    @brief Overwrites the current state-value function with a new one\n",
    "    '''\n",
    "    def update_value_function(self, V):\n",
    "        self.V = np.copy(V)\n",
    "\n",
    "    '''\n",
    "    @brief Returns the full state-value function V_pi\n",
    "    '''\n",
    "    def get_value_function(self):\n",
    "        return self.V\n",
    "\n",
    "    '''@brief Finds the optimal action for every state and updates the policy\n",
    "    '''\n",
    "    def update_greedy_policy(self):\n",
    "        # Note: We are assuming a greedy deterministic policy\n",
    "        self.pi_str = []\n",
    "        for i in range(self.env_size):\n",
    "            pi_row = []\n",
    "            for j in range(self.env_size):\n",
    "                if self.env.is_terminal_state(i,j):\n",
    "                    pi_row.append(\"X\")\n",
    "                    continue\n",
    "                    \n",
    "                _, self.pi_greedy[i,j], action_str = self.calculate_max_value(i, j)\n",
    "                pi_row.append(action_str)\n",
    "            self.pi_str.append(pi_row)\n",
    "        \n",
    "    '''@brief Checks if there is the change in V is less than preset threshold\n",
    "    '''\n",
    "    def is_done(self, new_V):\n",
    "        delta = abs(self.V - new_V)\n",
    "        max_delta = delta.max()\n",
    "        return max_delta <= self.theta_threshold\n",
    "    \n",
    "    '''@brief Returns the stored greedy policy\n",
    "    '''\n",
    "    def get_policy(self):\n",
    "        return self.pi_greedy\n",
    "    \n",
    "    '''@brief Prints the policy using the action descriptions\n",
    "    '''\n",
    "    def print_policy(self):\n",
    "        for row in self.pi_str:\n",
    "            print(row)\n",
    "\n",
    "\n",
    "def run_mdp():\n",
    "    ENV_SIZE = 5\n",
    "    THETA_THRESHOLD = 0.05\n",
    "    MAX_ITERATIONS = 1000\n",
    "    env = GridWorld(ENV_SIZE)\n",
    "    agent = Agent(env, THETA_THRESHOLD)\n",
    "\n",
    "    # Perform value iteration\n",
    "    done = False\n",
    "    for iter in range(MAX_ITERATIONS):\n",
    "        # Add stopping criteria if change in value function is small\n",
    "        if done: break\n",
    "        # Make a copy of the value function\n",
    "        # TODO: Try in-place state-value function update where Vpi is updated with every state\n",
    "        new_V = np.copy(agent.get_value_function())\n",
    "        # Loop over all states\n",
    "        for i in range(ENV_SIZE):\n",
    "            for j in range(ENV_SIZE):\n",
    "                if not env.is_terminal_state(i, j):\n",
    "                    new_V[i, j], _, _= agent.calculate_max_value(i,j)\n",
    "        # TODO: Uncomment the next line and compare how many iterations it takes\n",
    "        # TODO: Change the theta_threshold value to a large value (1.0) and explore what happens to the optimal state-value function and policy\n",
    "        # done = agent.is_done(new_V)\n",
    "        agent.update_value_function(new_V)\n",
    "\n",
    "    # Print the optimal value function\n",
    "    print(\"Optimal Value Function Found in %d iterations:\"%(iter+1))\n",
    "    print(agent.get_value_function())\n",
    "\n",
    "    agent.update_greedy_policy()\n",
    "    agent.print_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Value Function Found in 1000 iterations:\n",
      "[[ 3.  4.  5.  6.  7.]\n",
      " [ 4.  5.  6.  7.  8.]\n",
      " [ 5.  6.  7.  8.  9.]\n",
      " [ 6.  7.  8.  9. 10.]\n",
      " [ 7.  8.  9. 10.  0.]]\n",
      "['Right|Down', 'Right|Down', 'Right|Down', 'Down', 'Down']\n",
      "['Right|Down', 'Right|Down', 'Right', 'Right|Down', 'Down']\n",
      "['Right', 'Down', 'Right|Down', 'Right|Down', 'Down']\n",
      "['Right|Down', 'Right|Down', 'Right|Down', 'Right|Down', 'Down']\n",
      "['Right', 'Right', 'Right', 'Right', 'X']\n"
     ]
    }
   ],
   "source": [
    "run_mdp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Modified the code to add the reward functions as described in the question can be found in GridWorld class.\n",
    "```python \n",
    "####### EDITED FOR THE TASK - Nikhil Shankar#####\n",
    "        ## Incorporating various rewards for the states\n",
    "        # Assign a vector of rewards for each of the states\n",
    "        self.reward = np.ones((self.env_size, self.env_size))*-1\n",
    "        self.reward[self.terminal_state] = 10 \n",
    "        self.reward[(2,2)] = -5\n",
    "        self.reward[(3,0)] = -5\n",
    "        self.reward[(0,4)] = -5\n",
    "####### EDITED FOR THE TASK - Nikhil Shankar#####\n",
    "```\n",
    "\n",
    "> The modified code can be found in the GridWorld class.\n",
    "\n",
    "\n",
    "- If you see the Policy output between the previous output and the one above you can see the difference.\n",
    "- Check policy for (1,2). Before rewards it was **Right|Down** after incorporating rewards it is **Right** which is better since if we move down we end up in a gray state which we want to avoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Task 3.1.1 Value Table**\n",
    "\n",
    "| | | | | |\n",
    "|-|-|-|-|-| \n",
    "| 3 | 4 | 5 | 6 | 7 |\n",
    "| 4 | 5 | 6 | 7 | 8 |\n",
    "| 5 | 6 | 7 | 8 | 9 |\n",
    "| 6 | 7 | 8 | 9 | 10 |\n",
    "| 7 | 8 | 9 | 10 | 0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Task 3.1.2 Policy Table**\n",
    "\n",
    "| | | | | |\n",
    "|-|-|-|-|-| \n",
    "|'Right/Down' | 'Right/Down' | 'Right/Down' | 'Down' | 'Down'|\n",
    "|'Right/Down' | 'Right/Down' | 'Right' | 'Right/Down' | 'Down'|\n",
    "|'Right' | 'Down' | 'Right/Down' | 'Right/Down' | 'Down' |\n",
    "| 'Right|Down' | 'Right/Down' | 'Right/Down' | 'Right/Down' | 'Down' |\n",
    "| 'Right' | 'Right' | 'Right' | 'Right' | 'X' |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Problem 3: Task 2 : Value Iteration Variations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "#!/usr/bin/python3\n",
    "import numpy as np\n",
    "\n",
    "class GridWorld():\n",
    "    def __init__(self, env_size):\n",
    "        self.env_size = env_size\n",
    "        # TODO: Change the location of the terminal state and check how the optimal policy changes\n",
    "        # TODO: Add more than one terminal state (requires more changes in the code)\n",
    "        self.terminal_state = (4, 4)\n",
    "\n",
    "        # Define the transition probabilities and rewards\n",
    "        self.actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # Right, Left, Down, Up\n",
    "        self.action_description = [\"Right\", \"Left\", \"Down\", \"Up\"]\n",
    "\n",
    "        \n",
    "\n",
    "        ####### EDITED FOR THE TASK - Nikhil Shankar#####\n",
    "        ## Incorporating various rewards for the states\n",
    "        # Assign a vector of rewards for each of the states\n",
    "        self.reward = np.ones((self.env_size, self.env_size))*-1\n",
    "        self.reward[self.terminal_state] = 10 \n",
    "        self.reward[(2,2)] = -5\n",
    "        self.reward[(3,0)] = -5\n",
    "        self.reward[(0,4)] = -5\n",
    "        ####### EDITED FOR THE TASK - Nikhil Shankar#####\n",
    "\n",
    "    '''@brief Returns the next state given the chosen action and current state\n",
    "    '''\n",
    "    def step(self, action_index, i, j):\n",
    "        # We are assuming a Transition Probability Matrix where\n",
    "        # P(s'|s) = 1.0 for a single state and 0 otherwise\n",
    "        action = self.actions[action_index]\n",
    "        next_i, next_j = i + action[0], j + action[1]\n",
    "        if not self.is_valid_state(next_i, next_j):\n",
    "            next_i, next_j = i, j\n",
    "        \n",
    "        done = self.is_terminal_state(next_i, next_j)\n",
    "        reward = self.reward[next_i, next_j]\n",
    "        return next_i, next_j, reward, done\n",
    "    \n",
    "    '''@brief Checks if a state is within the acceptable bounds of the environment\n",
    "    '''\n",
    "    def is_valid_state(self, i, j):\n",
    "        valid = 0 <= i < self.env_size and 0 <= j < self.env_size\n",
    "        return valid\n",
    "    \n",
    "    '''@brief Returns True if the state is a terminal state\n",
    "    '''\n",
    "    def is_terminal_state(self, i, j):\n",
    "        return (i, j) == self. terminal_state\n",
    "    \n",
    "    def get_size(self):\n",
    "        return self.env_size\n",
    "\n",
    "    def get_actions(self):\n",
    "        return self.actions\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, env, theta_threshold=0.01):\n",
    "        self.env_size = env.get_size()\n",
    "        self.env = env\n",
    "        # Initialize the value function and set terminal state value to 0\n",
    "        self.V = np.zeros((self.env_size, self.env_size))\n",
    "        # TODO: Change the location of the terminal state and check how the optimal policy changes\n",
    "        # TODO: Add more than one terminal state (requires more changes in the code)\n",
    "        self.terminal_state = (4, 4)\n",
    "        self.V[self.terminal_state] = 0\n",
    "\n",
    "        self.theta_threshold = theta_threshold\n",
    "\n",
    "        # Define the transition probabilities and rewards\n",
    "        self.actions = env.get_actions()  # Right, Left, Down, Up\n",
    "        self.gamma = 1.0  # Discount factor\n",
    "        self.pi_greedy = np.zeros((self.env_size, self.env_size), dtype=int)\n",
    "\n",
    "    '''@brief Calculate the maximim value by following a greedy policy\n",
    "    '''\n",
    "    def calculate_max_value(self, i, j):\n",
    "        # Find the maximum value for the current state using Bellman's equation\n",
    "        # Start with a - infinite value as the max\n",
    "        max_value = float('-inf')\n",
    "        best_action = None\n",
    "        best_actions_str = \"\"\n",
    "        # Loop over all actions\n",
    "        for action_index in range(len(self.actions)):\n",
    "          # Find Next state\n",
    "            next_i, next_j, reward, _ = self.env.step(action_index, i, j)\n",
    "            if self.env.is_valid_state(next_i, next_j):\n",
    "                value = self.get_value(next_i, next_j, reward)\n",
    "                if value >= max_value:\n",
    "                    # Populating the best_actions description string\n",
    "                    if value > max_value:\n",
    "                        best_actions_str = self.env.action_description[action_index]\n",
    "                    else:\n",
    "                        best_actions_str += \"|\" + self.env.action_description[action_index]\n",
    "\n",
    "                    best_action = action_index\n",
    "                    max_value = value\n",
    "        return max_value, best_action, best_actions_str\n",
    "    \n",
    "    '''@brief use the Bellman equation to calculate the value of a single state\n",
    "            Note that the equation is simplified due to the simple transition matrix\n",
    "    '''\n",
    "    def get_value(self, i, j, reward):\n",
    "        return reward + self.gamma * self.V[i, j]\n",
    "    \n",
    "    '''\n",
    "    @brief Overwrites the current state-value function with a new one\n",
    "    '''\n",
    "    def update_value_function(self, V):\n",
    "        self.V = np.copy(V)\n",
    "\n",
    "    '''\n",
    "    @brief Returns the full state-value function V_pi\n",
    "    '''\n",
    "    def get_value_function(self):\n",
    "        return self.V\n",
    "\n",
    "    '''@brief Finds the optimal action for every state and updates the policy\n",
    "    '''\n",
    "    def update_greedy_policy(self):\n",
    "        # Note: We are assuming a greedy deterministic policy\n",
    "        self.pi_str = []\n",
    "        for i in range(self.env_size):\n",
    "            pi_row = []\n",
    "            for j in range(self.env_size):\n",
    "                if self.env.is_terminal_state(i,j):\n",
    "                    pi_row.append(\"X\")\n",
    "                    continue\n",
    "                    \n",
    "                _, self.pi_greedy[i,j], action_str = self.calculate_max_value(i, j)\n",
    "                pi_row.append(action_str)\n",
    "            self.pi_str.append(pi_row)\n",
    "        \n",
    "    '''@brief Checks if there is the change in V is less than preset threshold\n",
    "    '''\n",
    "    def is_done(self, new_V):\n",
    "        delta = abs(self.V - new_V)\n",
    "        max_delta = delta.max()\n",
    "        return max_delta <= self.theta_threshold\n",
    "    \n",
    "    '''@brief Returns the stored greedy policy\n",
    "    '''\n",
    "    def get_policy(self):\n",
    "        return self.pi_greedy\n",
    "    \n",
    "    '''@brief Prints the policy using the action descriptions\n",
    "    '''\n",
    "    def print_policy(self):\n",
    "        for row in self.pi_str:\n",
    "            print(row)\n",
    "\n",
    "\n",
    "def run_mdp():\n",
    "    print(\"Running problem 3 task 2\")\n",
    "    ENV_SIZE = 5\n",
    "    THETA_THRESHOLD = 0.05\n",
    "    MAX_ITERATIONS = 1000\n",
    "    env = GridWorld(ENV_SIZE)\n",
    "    agent = Agent(env, THETA_THRESHOLD)\n",
    "\n",
    "    # Perform value iteration\n",
    "    done = False\n",
    "    for iter in range(MAX_ITERATIONS):\n",
    "        # Add stopping criteria if change in value function is small\n",
    "        if done: break\n",
    "        # Make a copy of the value function\n",
    "        # TODO: Try in-place state-value function update where Vpi is updated with every state\n",
    "        #new_V = np.copy(agent.get_value_function())\n",
    "        # Loop over all states\n",
    "\n",
    "        ### Updated code for task problem 3 task 2\n",
    "        for i in range(ENV_SIZE):\n",
    "            for j in range(ENV_SIZE):\n",
    "                if not env.is_terminal_state(i, j):\n",
    "                    #if i == 2 and j == 2:\n",
    "                        #print(f\"Before {agent.get_value_function()[i, j]}\")\n",
    "                    agent.get_value_function()[i, j], _, _= agent.calculate_max_value(i,j)\n",
    "                    #if i == 2 and j == 2:\n",
    "                        #print(f\"After {agent.get_value_function()[i, j]}\")\n",
    "        ### Updated code for task problem 3 task 2\n",
    "        # TODO: Uncomment the next line and compare how many iterations it takes\n",
    "        # TODO: Change the theta_threshold value to a large value (1.0) and explore what happens to the optimal state-value function and policy\n",
    "        # done = agent.is_done(new_V)\n",
    "        #agent.update_value_function(new_V)\n",
    "\n",
    "    # Print the optimal value function\n",
    "    print(\"Optimal Value Function Found in %d iterations:\"%(iter+1))\n",
    "    print(agent.get_value_function())\n",
    "\n",
    "    agent.update_greedy_policy()\n",
    "    agent.print_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running problem 3 task 2\n",
      "Optimal Value Function Found in 1000 iterations:\n",
      "[[ 3.  4.  5.  6.  7.]\n",
      " [ 4.  5.  6.  7.  8.]\n",
      " [ 5.  6.  7.  8.  9.]\n",
      " [ 6.  7.  8.  9. 10.]\n",
      " [ 7.  8.  9. 10.  0.]]\n",
      "['Right|Down', 'Right|Down', 'Right|Down', 'Down', 'Down']\n",
      "['Right|Down', 'Right|Down', 'Right', 'Right|Down', 'Down']\n",
      "['Right', 'Down', 'Right|Down', 'Right|Down', 'Down']\n",
      "['Right|Down', 'Right|Down', 'Right|Down', 'Right|Down', 'Down']\n",
      "['Right', 'Right', 'Right', 'Right', 'X']\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "run_mdp()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Updating the value function to update immediately after each value state updation doesn't seem to affect the outcome.\n",
    "- Code changes \n",
    "\n",
    "```python\n",
    "### Updated code for task problem 3 task 2\n",
    "       for i in range(ENV_SIZE):\n",
    "            for j in range(ENV_SIZE):\n",
    "                if not env.is_terminal_state(i, j):\n",
    "                    if i == 2 and j == 2:\n",
    "                        print(f\"Before {agent.get_value_function()[i, j]}\")\n",
    "                    agent.get_value_function()[i, j], _, _= agent.calculate_max_value(i,j)\n",
    "                    if i == 2 and j == 2:\n",
    "                        print(f\"After {agent.get_value_function()[i, j]}\")\n",
    "### Updated code for task problem 3 task 2\n",
    "```\n",
    "\n",
    "- Added the print statement to check and verify that the value function is getting updated inplace and to avoid printing a lot of statements added i == 2 and j == 2 check as well.\n",
    "- We are updating the value function inside the loop iteration itself. Previously it was done outside the loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Calculate time and Memory usage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running problem 3 task 2\n",
      "Optimal Value Function Found in 1000 iterations:\n",
      "[[ 3.  4.  5.  6.  7.]\n",
      " [ 4.  5.  6.  7.  8.]\n",
      " [ 5.  6.  7.  8.  9.]\n",
      " [ 6.  7.  8.  9. 10.]\n",
      " [ 7.  8.  9. 10.  0.]]\n",
      "['Right|Down', 'Right|Down', 'Right|Down', 'Down', 'Down']\n",
      "['Right|Down', 'Right|Down', 'Right', 'Right|Down', 'Down']\n",
      "['Right', 'Down', 'Right|Down', 'Right|Down', 'Down']\n",
      "['Right|Down', 'Right|Down', 'Right|Down', 'Right|Down', 'Down']\n",
      "['Right', 'Right', 'Right', 'Right', 'X']\n",
      "Execution Time: 0.549248 seconds\n",
      "Peak Memory Usage: 0.014581 MB\n"
     ]
    }
   ],
   "source": [
    "import tracemalloc\n",
    "\n",
    "start_time = time.time()\n",
    "tracemalloc.start()  # Start memory tracking\n",
    " # Record start time\n",
    "result = run_mdp()\n",
    "end_time = time.time()  # Record end time\n",
    "current, peak = tracemalloc.get_traced_memory()\n",
    "tracemalloc.stop()\n",
    "\n",
    "print(f\"Execution Time: {end_time - start_time:.6f} seconds\")\n",
    "print(f\"Peak Memory Usage: {peak / 1024 / 1024:.6f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Both takes around ~0.54 seconds and ~0.014MB \n",
    "- There was no significant improvements in time nor Memory usage after doing the calculations in place.\n",
    "- Ideally we should have gained some memory improvements due to the inplace calculations we have done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Problem 4 - Task 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Layout and Policy:\n",
      "-----------\n",
      "|→|→|→|↓|H|\n",
      "-----------\n",
      "|→|→|→|↓|←|\n",
      "-----------\n",
      "|↓|↑|H|↓|↓|\n",
      "-----------\n",
      "|H|↓|↓|↓|↓|\n",
      "-----------\n",
      "|→|→|→|→|G|\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self):\n",
    "        self.grid_size = 5\n",
    "        self.goal = (4, 4)\n",
    "        self.holes = [(3, 0), (2, 2), (0, 4)]\n",
    "        # Action indices: 0=right, 1=down, 2=left, 3=up\n",
    "        self.actions = [(0, 1), (1, 0), (0, -1), (-1, 0)]\n",
    "        \n",
    "    def get_reward(self, state, next_state):\n",
    "        if next_state == self.goal:\n",
    "            return 10\n",
    "        elif next_state in self.holes:\n",
    "            return -5\n",
    "        return -1\n",
    "    \n",
    "    def is_terminal(self, state):\n",
    "        return state == self.goal or state in self.holes\n",
    "    \n",
    "    def next_state(self, state, action):\n",
    "        x, y = state\n",
    "        dx, dy = action\n",
    "        next_x, next_y = x + dx, y + dy\n",
    "        \n",
    "        # If the next position would be outside the grid, stay in current position\n",
    "        if not (0 <= next_x < self.grid_size and 0 <= next_y < self.grid_size):\n",
    "            return state\n",
    "            \n",
    "        return (next_x, next_y)\n",
    "\n",
    "def epsilon_greedy_policy(Q, state, epsilon=0.1):\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.randint(4)\n",
    "    else:\n",
    "        return np.argmax(Q[state])\n",
    "\n",
    "def generate_episode(env, Q, epsilon):\n",
    "    episode = []\n",
    "    state = (0, 0)  # Start state\n",
    "    \n",
    "    while not env.is_terminal(state):\n",
    "        # Choose action using epsilon-greedy policy\n",
    "        action_idx = epsilon_greedy_policy(Q, state, epsilon)\n",
    "        action = env.actions[action_idx]\n",
    "        \n",
    "        # Take action and observe next state and reward\n",
    "        next_state = env.next_state(state, action)\n",
    "        reward = env.get_reward(state, next_state)\n",
    "        \n",
    "        episode.append((state, action_idx, reward, next_state))\n",
    "        state = next_state\n",
    "        \n",
    "        if len(episode) > 100:  # Prevent infinite loops\n",
    "            break\n",
    "            \n",
    "    return episode\n",
    "\n",
    "def monte_carlo_off_policy(num_episodes=5000, gamma=0.9, epsilon=0.1):\n",
    "    env = GridWorld()\n",
    "    \n",
    "    # Initialize Q-values and returns\n",
    "    Q = defaultdict(lambda: np.zeros(len(env.actions)))\n",
    "    returns = defaultdict(list)  # Store returns for each state-action pair\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # Generate episode using epsilon-greedy policy\n",
    "        trajectory = generate_episode(env, Q, epsilon)\n",
    "        \n",
    "        # Calculate returns for each step\n",
    "        G = 0\n",
    "        for t in range(len(trajectory)-1, -1, -1):\n",
    "            state, action, reward, next_state = trajectory[t]\n",
    "            G = gamma * G + reward\n",
    "            \n",
    "            # Store return for this state-action pair\n",
    "            state_action = (state, action)\n",
    "            returns[state_action].append(G)\n",
    "            \n",
    "            # Update Q-value with average return\n",
    "            Q[state][action] = np.mean(returns[state_action])\n",
    "    \n",
    "    # Create deterministic policy from Q-values\n",
    "    policy = {}\n",
    "    for state in Q.keys():\n",
    "        policy[state] = np.argmax(Q[state])\n",
    "    \n",
    "    return Q, policy\n",
    "\n",
    "# Run the algorithm\n",
    "Q, policy = monte_carlo_off_policy(num_episodes=20000, gamma=0.9, epsilon=0.1)\n",
    "\n",
    "def print_policy_and_grid(policy, grid_size=5):\n",
    "    arrows = ['→', '↓', '←', '↑']\n",
    "    print(\"\\nGrid Layout and Policy:\")\n",
    "    print(\"-\" * (grid_size * 2 + 1))\n",
    "    \n",
    "    for i in range(grid_size):\n",
    "        print(\"|\", end=\"\")\n",
    "        for j in range(grid_size):\n",
    "            if (i, j) == (4, 4):\n",
    "                print(\"G\", end=\"|\")\n",
    "            elif (i, j) in [(3, 0), (2, 2), (0, 4)]:\n",
    "                print(\"H\", end=\"|\")\n",
    "            else:\n",
    "                if (i, j) in policy:\n",
    "                    print(arrows[policy[(i, j)]], end=\"|\")\n",
    "                else:\n",
    "                    print(\" \", end=\"|\")\n",
    "        print(\"\\n\" + \"-\" * (grid_size * 2 + 1))\n",
    "\n",
    "# Print the learned policy\n",
    "print_policy_and_grid(policy)\n",
    "\n",
    "def print_q_values(Q, state=(0, 0)):\n",
    "    print(f\"\\nQ-values for state {state}:\")\n",
    "    actions = ['Right', 'Down', 'Left', 'Up']\n",
    "    for i, action in enumerate(actions):\n",
    "        print(f\"{action}: {Q[state][i]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 0\n",
      "| [-1.42953202 -2.77273959 -2.87170114 -4.4528316 ] |\n",
      "| [-0.36432429 -1.12715391 -2.87431111 -1.6702724 ] |\n",
      "| [ 0.79891891  0.66575352 -1.54286064 -0.62582355] |\n",
      "| [-5.          2.28465252 -0.33298404  0.77624611] |\n",
      "| [0. 0. 0. 0.] |\n",
      "\n",
      "\n",
      "Row 1\n",
      "| [-1.02359178 -7.07926226 -8.28196816 -7.82454021] |\n",
      "| [ 0.81248739 -2.14713743 -3.21694826 -1.5892916 ] |\n",
      "| [ 2.27627236 -5.         -0.82901674 -0.22466618] |\n",
      "| [0.92556978 3.87983075 0.70519031 0.72438273] |\n",
      "| [-2.37140424  1.40555175  2.35516086 -5.        ] |\n",
      "\n",
      "\n",
      "Row 2\n",
      "| [-6.93678084 -5.         -6.92624904 -8.97153745] |\n",
      "| [-5.         -3.43869148 -7.74875921 -0.72403472] |\n",
      "| [0. 0. 0. 0.] |\n",
      "| [ 4.02359917  5.82128794 -5.          2.19698384] |\n",
      "| [-5.16808381  7.66142818  3.71711027 -1.11875503] |\n",
      "\n",
      "\n",
      "Row 3\n",
      "| [0. 0. 0. 0.] |\n",
      "| [-2.7096572  3.84128    0.         0.       ] |\n",
      "| [ 4.58007618  5.77019702  2.457152   -5.        ] |\n",
      "| [7.60117184 7.76861977 4.02809698 3.82709756] |\n",
      "| [-1.62290427 10.          6.2         3.2604497 ] |\n",
      "\n",
      "\n",
      "Row 4\n",
      "| [4.58 0.   0.   0.  ] |\n",
      "| [5.77781818 4.58       3.122      0.        ] |\n",
      "| [7.75494719 5.603894   4.2641     4.03483478] |\n",
      "| [10.          7.72437863  5.81750479  5.83926406] |\n",
      "| [0. 0. 0. 0.] |\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 5):\n",
    "    print(f\"Row {i}\")\n",
    "    for j in range(0,5):\n",
    "        value = Q[(i,j)]\n",
    "        print(f\"| {value} |\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We printed the value function for all actions in each state. We can see that the action with the highest value is what we need to take at each step.\n",
    "\n",
    "- eg. In Row4 4th entry = S(4,3) = [10.          7.72437863  5.81750479  5.83926406]\n",
    "\n",
    "- Ideally condition the action to be taken is to move right from there to reach the goal. \n",
    "```python\n",
    "self.actions = [(0, 1), (1, 0), (0, -1), (-1, 0)]\n",
    "```\n",
    "- This is the definiton of our actions. And checking the values for S(4,3) we can see that the highest value is correctly assigned to the action **Right** which is 10.\n",
    "\n",
    "- Now we can go ahead and output value function for each state and corresponding action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_arg_max_value(Q):\n",
    "    for i in range(0, 5):\n",
    "        val = \"| \"\n",
    "        for j in range(0,5):\n",
    "            value = np.max(Q[(i,j)])\n",
    "            val += f\" {round(value,2)} \"\n",
    "        print(val + \" |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Actions\n",
      "\n",
      "Grid Layout and Policy:\n",
      "-----------\n",
      "|→|→|→|↓|H|\n",
      "-----------\n",
      "|→|→|→|↓|←|\n",
      "-----------\n",
      "|↓|↑|H|↓|↓|\n",
      "-----------\n",
      "|H|↓|↓|↓|↓|\n",
      "-----------\n",
      "|→|→|→|→|G|\n",
      "-----------\n",
      "Optimal Value function\n",
      "|  -1.43  -0.36  0.8  2.28  0.0  |\n",
      "|  -1.02  0.81  2.28  3.88  2.36  |\n",
      "|  -5.0  -0.72  0.0  5.82  7.66  |\n",
      "|  0.0  3.84  5.77  7.77  10.0  |\n",
      "|  4.58  5.78  7.75  10.0  0.0  |\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal Actions\")\n",
    "print_policy_and_grid(policy)\n",
    "\n",
    "print(\"Optimal Value function\")\n",
    "print_arg_max_value(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Comparison Monte carlo vs Markov Decision Process** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Monte carlo method took drastically higher time to find the optimal output\n",
    "- Monte carlo took ~90 seconds while MDP was able to do that in less than a second.\n",
    "- With smaller number of episodes Monte Carlo struggled to arrive at the optimum policy. ( Tried with 1000, 5000 and 10000)\n",
    "- So computationally Monte Carlo is inferior to MDP method.\n",
    "- Monte carlo can work even without reward (We have used rewards for our use case ) or probabilities whereas MDP requires it.\n",
    "- MC is useful when model dynamics are difficult to define.\n",
    "- MDP is quick but it require model and environment dynamics so it is suited to solve problems which are well defined.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSCN8010_classical_ml_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
